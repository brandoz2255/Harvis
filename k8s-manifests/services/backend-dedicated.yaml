apiVersion: apps/v1
kind: Deployment
metadata:
  name: harvis-ai-backend
  namespace: ai-agents
  labels:
    app.kubernetes.io/name: harvis-ai
    app.kubernetes.io/component: backend
  annotations:
    # Flux image automation annotations
    fluxcd.io/automated: "true"
    fluxcd.io/tag.harvis-backend: semver:~1.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: harvis-ai
      app.kubernetes.io/component: backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: harvis-ai
        app.kubernetes.io/component: backend
    spec:
      runtimeClassName: nvidia
      # Node affinity: Force scheduling on Node 1 (pop-os) control-plane for ML workload
      nodeSelector:
        kubernetes.io/hostname: pop-os
      initContainers:
        # Init container to pre-download TTS and Whisper models
        - name: download-models
          image: "localhost:5000/jarvis-backend:v1.3v3"
          imagePullPolicy: Always
          command: ["/app/entrypoint.sh", "download-models"]
          env:
            # Ensure init container uses same cache paths
            - name: HF_HOME
              value: "/models-cache/huggingface"
            - name: TRANSFORMERS_CACHE
              value: "/models-cache/huggingface"
            - name: WHISPER_CACHE
              value: "/models-cache/whisper"
          volumeMounts:
            - name: ml-models-cache
              mountPath: /models-cache
          resources:
            requests:
              memory: 2Gi
              cpu: 500m
            limits:
              memory: 4Gi
              cpu: 2000m
      containers:
        # Harvis Backend container with full GPU for ML workloads
        - name: harvis-backend
          image: "localhost:5000/jarvis-backend:latest" # {"$imagepolicy": "flux-system:harvis-backend-prod"}
          imagePullPolicy: Always
          resources:
            limits:
              nvidia.com/gpu: 1  # Full GPU for ML workloads (Whisper, TTS, Vision)
            requests:
              memory: 12Gi
              cpu: 2000m
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: harvis-ai-backend-secret
                  key: database-url
            - name: OLLAMA_API_KEY
              valueFrom:
                secretKeyRef:
                  name: harvis-ai-backend-secret
                  key: ollama-api-key
            # Point to dedicated Ollama service on Node 1
            - name: OLLAMA_URL
              value: "http://harvis-ai-ollama:11434"
            - name: BACKEND_URL
              value: "http://harvis-ai-backend:8000"
            # Model cache configuration - HF_HOME is preferred by transformers library
            - name: HF_HOME
              value: "/models-cache/huggingface"
            - name: TRANSFORMERS_CACHE
              value: "/models-cache/huggingface"
            - name: WHISPER_CACHE
              value: "/models-cache/whisper"
            # Production uvicorn settings
            - name: UVICORN_WORKERS
              value: "1"
            - name: UVICORN_TIMEOUT_KEEP_ALIVE
              value: "120"
          envFrom:
            - configMapRef:
                name: harvis-ai-backend-config
            - secretRef:
                name: harvis-ai-backend-secret
          volumeMounts:
            # - name: embedding
            #   mountPath: /app/embedding
            - name: tmp
              mountPath: /tmp
            - name: ml-models-cache
              mountPath: /models-cache
          command: ["uvicorn"]
          args:
            - "main:app"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"
            - "--timeout-keep-alive"
            - "120"
            - "--log-level"
            - "info"
          livenessProbe:
            httpGet:
              path: /docs
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /docs
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5

      volumes:
        # - name: embedding
        #   persistentVolumeClaim:
        #     claimName: harvis-ai-embedding-pvc
        - name: ml-models-cache
          persistentVolumeClaim:
            claimName: ml-models-local-pvc  # Local storage on node1 for fast model access
        - name: tmp
          persistentVolumeClaim:
            claimName: harvis-audio-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: harvis-ai-backend
  namespace: ai-agents
  labels:
    app.kubernetes.io/name: harvis-ai
    app.kubernetes.io/component: backend
spec:
  type: ClusterIP
  ports:
    - name: backend
      port: 8000
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: harvis-ai
    app.kubernetes.io/component: backend
